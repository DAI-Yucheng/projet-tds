# -*- coding: utf-8 -*-
"""Son_TP4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xXjuod9emn0F1XqqNIvrO3gdRa-EAFqt

# TP traitement avancé des sons - Séparation de sources
Implementation de réseau UNet pour la séparation de voix chantées.

Auteurs : Hicham TIERCE, Maria YURYEVA
"""

pip install musdb

pip install museval

"""Imports"""

import numpy as np
import librosa
import musdb
import museval
import random
from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, LeakyReLU, Dropout, Activation, Concatenate, Conv2DTranspose
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import tensorflow as tf

#Récupération de la base de données complète dans Google Drive
from google.colab import drive
import os

drive.mount('/content/drive')

music_folder_path = '/content/drive/MyDrive/musdb18' #lien vers la base de données qui est dans mon google drive

#vérification
if os.path.exists(music_folder_path):
    print(f"Dossier trouvé ! Nombre d'éléments : {len(os.listdir(music_folder_path))}")
else:
    print("Chemin introuvable")

mus = musdb.DB(root=music_folder_path)
mus[0].audio

"""config"""

WINDOW_SIZE = 1024
SAMPLE_RATE = 8192
HOP_LENGTH = 768
N_FFT = 1024
PATCH_SIZE = 128  # 128 frames per spectrogram patch
BATCH_SIZE = 16
EPOCHS = 10

"""

## Data processing"""

def compute_spectrogram(audio, sr=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH):
    """
    Compute magnitude spectrogram from audio
    Returns: (freq_bins, time_frames) magnitude spectrogram
    Note: We use only first 512 bins (instead of 513) for power-of-2 dimensions
    """
    # Resample if needed
    if sr != SAMPLE_RATE:
        audio = librosa.resample(audio, orig_sr=sr, target_sr=SAMPLE_RATE)

    # Compute STFT
    stft = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)
    magnitude = np.abs(stft)
    phase = np.angle(stft)

    # Use only first 512 frequency bins (remove DC and Nyquist for clean power-of-2)
    magnitude = magnitude[:512, :]
    phase = phase[:512, :]

    return magnitude, phase

def prepare_patch(magnitude, patch_size=PATCH_SIZE):
    """
    Split magnitude spectrogram into patches of fixed size
    Handles overlapping patches for full coverage
    """
    freq_bins, time_frames = magnitude.shape

    patches = []
    # Generate patches with stride
    stride = patch_size // 2  # 50% overlap to ensure coverage

    for i in range(0, time_frames - patch_size + 1, stride):
        patch = magnitude[:, i:i+patch_size]
        if patch.shape[1] == patch_size:  # Ensure full patch
            patches.append(patch)

    return np.array(patches)

"""## UNET Model

Code vient de https://github.com/Zhz1997/Singing-voice-speration-with-U-Net/blob/main/src/model.py
"""

def build_unet(input_shape=(512, 128, 1)):
    """
    UNet architecture for singing voice separation
    input_shape: (frequency_bins, time_frames, channels)
    Using 512 instead of 513 for clean division through the network
    """
    inputs = Input(input_shape)

    # Encoder
    conv1 = Conv2D(16, 5, strides=2, padding='same')(inputs)  # 256x64
    conv1 = BatchNormalization()(conv1)
    conv1 = LeakyReLU(negative_slope=0.2)(conv1)

    conv2 = Conv2D(32, 5, strides=2, padding='same')(conv1)  # 128x32
    conv2 = BatchNormalization()(conv2)
    conv2 = LeakyReLU(negative_slope=0.2)(conv2)

    conv3 = Conv2D(64, 5, strides=2, padding='same')(conv2)  # 64x16
    conv3 = BatchNormalization()(conv3)
    conv3 = LeakyReLU(negative_slope=0.2)(conv3)

    conv4 = Conv2D(128, 5, strides=2, padding='same')(conv3)  # 32x8
    conv4 = BatchNormalization()(conv4)
    conv4 = LeakyReLU(negative_slope=0.2)(conv4)

    conv5 = Conv2D(256, 5, strides=2, padding='same')(conv4)  # 16x4
    conv5 = BatchNormalization()(conv5)
    conv5 = LeakyReLU(negative_slope=0.2)(conv5)

    conv6 = Conv2D(512, 5, strides=2, padding='same')(conv5)  # 8x2
    conv6 = BatchNormalization()(conv6)
    conv6 = LeakyReLU(negative_slope=0.2)(conv6)

    # Decoder with skip connections
    deconv7 = Conv2DTranspose(256, 5, strides=2, padding='same')(conv6)  # 16x4
    deconv7 = BatchNormalization()(deconv7)
    deconv7 = Dropout(0.5)(deconv7)
    deconv7 = Activation('relu')(deconv7)

    deconv8 = Concatenate(axis=3)([deconv7, conv5])
    deconv8 = Conv2DTranspose(128, 5, strides=2, padding='same')(deconv8)  # 32x8
    deconv8 = BatchNormalization()(deconv8)
    deconv8 = Dropout(0.5)(deconv8)
    deconv8 = Activation('relu')(deconv8)

    deconv9 = Concatenate(axis=3)([deconv8, conv4])
    deconv9 = Conv2DTranspose(64, 5, strides=2, padding='same')(deconv9)  # 64x16
    deconv9 = BatchNormalization()(deconv9)
    deconv9 = Dropout(0.5)(deconv9)
    deconv9 = Activation('relu')(deconv9)

    deconv10 = Concatenate(axis=3)([deconv9, conv3])
    deconv10 = Conv2DTranspose(32, 5, strides=2, padding='same')(deconv10)  # 128x32
    deconv10 = BatchNormalization()(deconv10)
    deconv10 = Activation('relu')(deconv10)

    deconv11 = Concatenate(axis=3)([deconv10, conv2])
    deconv11 = Conv2DTranspose(16, 5, strides=2, padding='same')(deconv11)  # 256x64
    deconv11 = BatchNormalization()(deconv11)
    deconv11 = Activation('relu')(deconv11)

    deconv12 = Concatenate(axis=3)([deconv11, conv1])
    deconv12 = Conv2DTranspose(1, 5, strides=2, padding='same')(deconv12)  # 512x128
    deconv12 = Activation('sigmoid')(deconv12)  # Mask between 0 and 1

    model = Model(inputs=inputs, outputs=deconv12)
    return model

"""## Training"""

import time
from scipy.signal import resample_poly

def train_model(musdb_instance, epochs=EPOCHS, steps_per_epoch=100):
    """
    Version optimisée pour GPU avec suivi de progression
    """
    print("--- Initialisation de l'entraînement ---")

    # 1. Définition du générateur rapide (interne ou externe)
    def fast_generator():
        tracks = [t for t in musdb_instance.tracks if t.subset == 'train']
        buffer_size = 3 # Nombre de pistes chargées en mémoire à chaque cycle

        while True:
            all_x, all_y = [], []
            print(f"\n[CPU] Préparation d'un buffer de {buffer_size} pistes...")

            for _ in range(buffer_size):
                start = time.time()
                track = random.choice(tracks)

                # Chargement mono
                mix = track.audio.mean(axis=1)
                voc = track.targets['vocals'].audio.mean(axis=1)

                # Resampling rapide avec Scipy
                if track.rate != SAMPLE_RATE:
                    mix = resample_poly(mix, SAMPLE_RATE, track.rate)
                    voc = resample_poly(voc, SAMPLE_RATE, track.rate)

                # Spectrogrammes et Patches
                m_mag, _ = compute_spectrogram(mix)
                v_mag, _ = compute_spectrogram(voc)
                m_patches = prepare_patch(m_mag)
                v_patches = prepare_patch(v_mag)

                all_x.extend(m_patches)
                all_y.extend(v_patches)

                print(f"  > '{track.name[:20]}' traité en {time.time()-start:.2f}s | {len(m_patches)} patches")

            # Conversion et Shuffle
            X = np.array(all_x)[..., np.newaxis]
            Y = np.array(all_y)[..., np.newaxis]
            indices = np.arange(len(X))
            np.random.shuffle(indices)

            print(f"[GPU] Buffer prêt ({len(X)} patches total). Lancement des steps...")
            for i in range(0, len(X) - BATCH_SIZE + 1, BATCH_SIZE):
                yield X[indices[i:i+BATCH_SIZE]], Y[indices[i:i+BATCH_SIZE]]

    # 2. Création du Dataset TensorFlow pour le Prefetching
    output_signature = (
        tf.TensorSpec(shape=(None, 512, 128, 1), dtype=tf.float32),
        tf.TensorSpec(shape=(None, 512, 128, 1), dtype=tf.float32)
    )

    train_ds = tf.data.Dataset.from_generator(
        fast_generator,
        output_signature=output_signature
    ).prefetch(tf.data.AUTOTUNE) # Prépare le batch suivant pendant que le GPU travaille

    # 3. Build & Compile
    model = build_unet()
    model.compile(
        optimizer=Adam(learning_rate=0.0001),
        loss='mean_squared_error',
        metrics=['mae']
    )

    # 4. Entraînement
    history = model.fit(
        train_ds,
        steps_per_epoch=steps_per_epoch,
        epochs=epochs
    )

    return model, history

"""## Audio Reconstruction"""

def reconstruct_audio_from_spectrogram(magnitude, phase, hop_length=HOP_LENGTH):
    """
    Reconstruct audio from magnitude spectrogram using original phase
    """
    # Combine magnitude and phase
    stft_reconstructed = magnitude * np.exp(1j * phase)

    # Inverse STFT
    audio = librosa.istft(stft_reconstructed, hop_length=hop_length)

    return audio

def separate_vocals_from_track(model, track):
    """
    Separate vocals from a full track using the trained model
    """
    # Get full track audio
    mix_audio = track.audio.mean(axis=1)  # Stereo to mono
    mix_audio = librosa.resample(mix_audio, orig_sr=track.rate, target_sr=SAMPLE_RATE)

    # Compute spectrogram
    mix_mag, mix_phase = compute_spectrogram(mix_audio)

    # Create overlapping patches
    freq_bins, time_frames = mix_mag.shape
    stride = PATCH_SIZE // 2

    # Initialize output
    vocals_mag_full = np.zeros_like(mix_mag)
    count_matrix = np.zeros_like(mix_mag)  # For averaging overlaps

    # Process each patch
    for i in range(0, time_frames - PATCH_SIZE + 1, stride):
        patch = mix_mag[:, i:i+PATCH_SIZE]

        # Predict vocal mask
        patch_input = patch[np.newaxis, ..., np.newaxis]
        mask_pred = model.predict(patch_input, verbose=0)[0, :, :, 0]

        # Apply mask to get vocals magnitude
        vocals_patch = mix_mag[:, i:i+PATCH_SIZE] * mask_pred

        # Add to output (for averaging overlaps)
        vocals_mag_full[:, i:i+PATCH_SIZE] += vocals_patch
        count_matrix[:, i:i+PATCH_SIZE] += 1

    # Average overlapping regions
    vocals_mag_full = vocals_mag_full / np.maximum(count_matrix, 1)

    # Reconstruct audio using mix phase
    vocals_audio = reconstruct_audio_from_spectrogram(vocals_mag_full, mix_phase)

    # Accompaniment = mix - vocals
    accompaniment_mag = mix_mag - vocals_mag_full
    accompaniment_audio = reconstruct_audio_from_spectrogram(accompaniment_mag, mix_phase)

    return vocals_audio, accompaniment_audio

"""## Evaluation"""

import pandas as pd

def evaluate_model(model, musdb_instance, n_tracks=5):
    test_tracks = [t for t in musdb_instance.tracks if t.subset == 'test']
    if n_tracks:
        test_tracks = test_tracks[:n_tracks]

    results = []

    for track in test_tracks:
        print(f"Évaluation de : {track.name}...")

        # Séparation
        vocals_audio, accompaniment_audio = separate_vocals_from_track(model, track)
        target_length = track.audio.shape[0]

        # Reconstruction Stéréo pour museval
        estimates = {
            'vocals': librosa.util.fix_length(np.stack([vocals_audio]*2, axis=1), size=target_length, axis=0),
            'accompaniment': librosa.util.fix_length(np.stack([accompaniment_audio]*2, axis=1), size=target_length, axis=0)
        }

        # Calcul des scores
        scores = museval.eval_mus_track(track, estimates)
        df = scores.df

        # Extraction des moyennes pour cette piste
        v_sdr = df[(df.target == 'vocals') & (df.metric == 'SDR')]['score'].mean()
        v_sir = df[(df.target == 'vocals') & (df.metric == 'SIR')]['score'].mean()
        v_sar = df[(df.target == 'vocals') & (df.metric == 'SAR')]['score'].mean()

        results.append({'track': track.name, 'SDR': v_sdr, 'SIR': v_sir, 'SAR': v_sar})
        print(f" > SDR: {v_sdr:.2f} | SIR: {v_sir:.2f} | SAR: {v_sar:.2f}")

    # Calcul de la moyenne générale pour le rapport de TP
    final_df = pd.DataFrame(results)
    print("\n" + "="*30)
    print("RÉSULTATS GLOBAUX DU MODÈLE")
    print("="*30)
    print(final_df[['SDR', 'SIR', 'SAR']].mean())
    return final_df

"""## Entrainement puis évaluation réelle"""

#entrainement
model, history = train_model(mus, epochs=10, steps_per_epoch=50)

#sauvegarde du modèle
model.save('unet_voice_separation.keras')

### Importer le modèle si on l'a déjà fait pour éviter de refaire un entrainement
model = tf.keras.models.load_model('unet_voice_separation.keras')
print("Modèle chargé avec succès !")
model.summary() # Pour vérifier que l'architecture est la bonne

#evaluation
scores = evaluate_model(model, mus)

from IPython.display import Audio, display

# Choisissons une piste du test set pour tester l'écoute
test_tracks = [t for t in mus.tracks if t.subset == 'test']
track = test_tracks[3]

print(f"Génération de l'audio pour : {track.name}")
vocals, accompaniment = separate_vocals_from_track(model, track)

print("--- Mix Original (Mono) ---")
mix_mono = track.audio.mean(axis=1)
display(Audio(mix_mono, rate=track.rate))

print("--- Vocaux Prédits ---")
display(Audio(vocals, rate=SAMPLE_RATE))

print("--- Accompagnement Prédit ---")
display(Audio(accompaniment, rate=SAMPLE_RATE))