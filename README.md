# Projet de SÃ©paration de Sources Vocales avec U-Net

## ğŸ“‹ Vue d'ensemble

Ce projet implÃ©mente un modÃ¨le U-Net pour sÃ©parer la voix (vocals) d'une chanson mixÃ©e. Le modÃ¨le apprend Ã  prÃ©dire un "masque" (mask) qui indique quelle partie du spectrogramme (reprÃ©sentation frÃ©quentielle de l'audio) correspond Ã  la voix.

**En termes simples** : On donne au modÃ¨le une chanson complÃ¨te (avec voix + instruments), et il nous dit "Ã  chaque moment et Ã  chaque frÃ©quence, quelle proportion est de la voix". On peut ensuite extraire uniquement la voix.

---

## ğŸ—‚ï¸ Structure du Projet et Relations entre les Fichiers

### Architecture gÃ©nÃ©rale

```
Audio (chanson mixÃ©e)
    â†“
data_generator.py  â†’  GÃ©nÃ¨re des donnÃ©es d'entraÃ®nement
    â†“
train.py  â†’  EntraÃ®ne le modÃ¨le U-Net
    â†“
unet_model.py  â†’  DÃ©finit l'architecture du modÃ¨le
    â†“
ModÃ¨le entraÃ®nÃ© (checkpoint)
    â†“
inference.py  â†’  Utilise le modÃ¨le pour sÃ©parer la voix
    â†“
Audio vocal extrait
```

### Fichiers principaux

#### 1. `data_generator.py` - **GÃ©nÃ©rateur de donnÃ©es (Ã‰tape 1)**

**RÃ´le** : Transforme les fichiers audio en donnÃ©es que le modÃ¨le peut comprendre.

**Ce qu'il fait** :
- Charge les chansons du dataset MUSDB (mix + voix sÃ©parÃ©e)
- Convertit l'audio en **spectrogramme** (reprÃ©sentation visuelle du son : frÃ©quence Ã— temps)
- DÃ©coupe en petits morceaux (patches) de 128 frames
- Normalise les donnÃ©es pour que le modÃ¨le puisse les traiter

**Termes importants** :
- **Spectrogramme** : ReprÃ©sentation 2D du son (frÃ©quence en vertical, temps en horizontal, intensitÃ© en couleur)
- **STFT** (Short-Time Fourier Transform) : MÃ©thode pour convertir l'audio en spectrogramme
- **Patch** : Un petit morceau du spectrogramme (128 frames temporelles)
- **Overlap** (chevauchement) : Les patches se chevauchent Ã  75% pour avoir plus de donnÃ©es d'entraÃ®nement

**Sortie** : Des batches de spectrogrammes normalisÃ©s (mix et voix)

---

#### 2. `unet_model.py` - **Architecture du modÃ¨le (Ã‰tape 2)**

**RÃ´le** : DÃ©finit la structure du rÃ©seau de neurones U-Net.

**Ce qu'il fait** :
- **Encoder** (encodeur) : RÃ©duit la taille du spectrogramme, extrait des caractÃ©ristiques
  - Utilise des **Conv2D** (convolutions 2D) avec stride=2 pour rÃ©duire la taille
  - Active avec **LeakyReLU** (fonction d'activation)
  
- **Decoder** (dÃ©codeur) : Reconstruit le spectrogramme Ã  la taille originale
  - Utilise des **ConvTranspose2D** (convolutions transposÃ©es) pour agrandir
  - **Skip connections** : Connecte les couches de l'encoder au decoder (comme un pont)
  - DerniÃ¨re couche : **Sigmoid** pour produire un masque entre 0 et 1

**Termes importants** :
- **U-Net** : Architecture de rÃ©seau en forme de "U" (rÃ©duction puis expansion)
- **Skip connections** : Connexions qui sautent des couches pour prÃ©server les dÃ©tails
- **Mask** (masque) : Matrice de valeurs entre 0 et 1, indiquant la proportion de voix Ã  chaque point

**EntrÃ©e** : Spectrogramme du mix (513 frÃ©quences Ã— 128 frames)  
**Sortie** : Mask (513 frÃ©quences Ã— 128 frames, valeurs entre 0 et 1)

---

#### 3. `train.py` - **Script d'entraÃ®nement (Ã‰tape 2)**

**RÃ´le** : EntraÃ®ne le modÃ¨le U-Net sur les donnÃ©es.

**Ce qu'il fait** :
1. Charge les donnÃ©es via `data_generator.py`
2. CrÃ©e le modÃ¨le via `unet_model.py`
3. DÃ©finit la **fonction de perte** (loss) : Oracle Mask Loss
   - Compare le mask prÃ©dit avec le mask "oracle" (vÃ©ritÃ© terrain)
   - **Oracle mask** = voix / mix (dans le domaine linÃ©aire)
4. EntraÃ®ne le modÃ¨le avec l'optimiseur Adam
5. Sauvegarde le meilleur modÃ¨le dans `checkpoints/`

**Termes importants** :
- **Loss** (perte) : Mesure de l'erreur du modÃ¨le (plus c'est bas, mieux c'est)
- **Epoch** : Un passage complet sur toutes les donnÃ©es
- **Batch** : Un groupe d'Ã©chantillons traitÃ©s ensemble
- **Learning rate** (taux d'apprentissage) : Vitesse Ã  laquelle le modÃ¨le apprend

**Utilisation** :
```bash
python train.py --epochs 20 --n-songs 10
```

---

#### 4. `inference.py` - **Script d'infÃ©rence (Utilisation)**

**RÃ´le** : Utilise un modÃ¨le entraÃ®nÃ© pour sÃ©parer la voix d'une nouvelle chanson.

**Ce qu'il fait** :
1. Charge un modÃ¨le entraÃ®nÃ© (checkpoint)
2. Charge un fichier audio (mix)
3. Convertit en spectrogramme (mÃªme mÃ©thode que l'entraÃ®nement)
4. Normalise (mÃªme mÃ©thode que l'entraÃ®nement)
5. Passe dans le modÃ¨le â†’ obtient le mask
6. Applique le mask au spectrogramme â†’ obtient le spectrogramme vocal
7. **Reconstruit l'audio** avec ISTFT (inverse de STFT)
8. Sauvegarde le fichier audio vocal

**Termes importants** :
- **Inference** (infÃ©rence) : Utiliser un modÃ¨le entraÃ®nÃ© pour faire des prÃ©dictions
- **ISTFT** : Inverse de STFT, reconvertit le spectrogramme en audio

**Utilisation** :
```bash
python inference.py --audio ma_chanson.wav
```

---

## ğŸ”„ Flux de DonnÃ©es Complet

### Phase 1 : PrÃ©paration des donnÃ©es

```
Fichiers audio MUSDB (mix.wav, vocals.wav)
    â†“
data_generator.py
    â”œâ”€ Charge l'audio
    â”œâ”€ Convertit en spectrogramme (STFT)
    â”œâ”€ DÃ©coupe en patches (128 frames)
    â”œâ”€ Calcule oracle_mask = vocals / mix
    â””â”€ Normalise
    â†“
Batches de donnÃ©es (mix_norm, oracle_mask)
```

### Phase 2 : EntraÃ®nement

```
Batches de donnÃ©es
    â†“
train.py
    â”œâ”€ CrÃ©e le modÃ¨le (unet_model.py)
    â”œâ”€ Passe mix_norm dans le modÃ¨le
    â”œâ”€ Obtient mask_prÃ©dit
    â”œâ”€ Compare avec oracle_mask (loss)
    â”œâ”€ Ajuste les poids du modÃ¨le (backpropagation)
    â””â”€ RÃ©pÃ¨te pour plusieurs epochs
    â†“
ModÃ¨le entraÃ®nÃ© (checkpoints/best_model.pth)
```

### Phase 3 : Utilisation

```
Nouvelle chanson (mix.wav)
    â†“
inference.py
    â”œâ”€ Charge le modÃ¨le entraÃ®nÃ©
    â”œâ”€ Convertit l'audio en spectrogramme
    â”œâ”€ Normalise
    â”œâ”€ Passe dans le modÃ¨le â†’ mask
    â”œâ”€ Applique mask au spectrogramme
    â”œâ”€ Reconstruit l'audio (ISTFT)
    â””â”€ Sauvegarde vocals.wav
    â†“
Fichier audio vocal extrait
```

---

## ğŸ“¦ Installation

### 1. DÃ©pendances systÃ¨me

**Important** : `musdb` nÃ©cessite `ffmpeg` pour traiter les fichiers audio.

```bash
# Ubuntu/WSL
sudo apt-get update
sudo apt-get install -y ffmpeg

# VÃ©rifier l'installation
ffmpeg -version
```

### 2. DÃ©pendances Python

```bash
# Installer PyTorch (avec CUDA si vous avez un GPU)
pip install torch torchvision torchaudio

# Installer les autres dÃ©pendances
pip install -r requirements.txt
```

### 3. Dataset MUSDB

**âš ï¸ IMPORTANT : TÃ©lÃ©chargement du Dataset**

Le dataset MUSDB18 complet doit Ãªtre **tÃ©lÃ©chargÃ© manuellement** depuis [Zenodo](https://zenodo.org/records/1117372) (4.7 GB).

**Pourquoi ?**
- `musdb.DB(download=True)` tÃ©lÃ©charge seulement la **version demo** (7 secondes par track)
- Le dataset complet contient des chansons longues (nÃ©cessaires pour l'entraÃ®nement)

**Instructions** :
1. TÃ©lÃ©charger depuis : https://zenodo.org/records/1117372
2. Extraire le fichier `musdb18.zip` (4.7 GB)
3. Placer le dossier `musdb18` dans : `/home/dyc/MUSDB18/`
4. Structure attendue : `/home/dyc/MUSDB18/musdb18/train/` et `/home/dyc/MUSDB18/musdb18/test/`

Le code dÃ©tectera automatiquement ce chemin. Si le dataset complet n'est pas trouvÃ©, il demandera confirmation avant de tÃ©lÃ©charger la version demo.

---

## ğŸš€ Utilisation

### Ã‰tape 1 : Tester le gÃ©nÃ©rateur de donnÃ©es

```bash
python data_generator.py
```

Cela vÃ©rifie que :
- Le dataset MUSDB est accessible
- Les spectrogrammes sont gÃ©nÃ©rÃ©s correctement
- Les patches ont la bonne taille (513 Ã— 128)

### Ã‰tape 2 : EntraÃ®ner le modÃ¨le

```bash
# EntraÃ®nement de base (20 epochs, 10 chansons)
python train.py

# Personnaliser les paramÃ¨tres
python train.py --epochs 20 --batch-size 16 --lr 5e-4 --n-songs 10

# Utiliser CPU (si pas de GPU)
python train.py --cpu
```

**ParamÃ¨tres recommandÃ©s** :
- `--epochs` : 10-20 (suffisant pour voir la convergence)
- `--batch-size` : 16 (ajustable selon votre mÃ©moire)
- `--lr` : 5e-4 (taux d'apprentissage)
- `--n-songs` : 5-10 (pour un entraÃ®nement rapide)

### Ã‰tape 3 : Utiliser le modÃ¨le pour sÃ©parer la voix

```bash
# Utiliser un fichier audio
python inference.py --audio ma_chanson.wav

# Utiliser le dataset MUSDB (premiÃ¨re chanson)
python inference.py
```

Le fichier vocal sera sauvegardÃ© avec le suffixe `_vocals.wav`.

---

## ğŸ“Š ParamÃ¨tres Techniques

### ParamÃ¨tres du spectrogramme (selon le papier)

- **Taux d'Ã©chantillonnage** : 8192 Hz (rÃ©duit de 44100 Hz pour accÃ©lÃ©rer)
- **Taille de fenÃªtre STFT** : 1024
- **Hop length** : 768
- **Taille de patch** : 128 frames
- **Overlap** : 75% (un patch tous les 32 frames)

### Architecture du modÃ¨le

- **FrÃ©quences** : 513 bins (1024/2 + 1)
- **Frames temporelles** : 128
- **Canaux initiaux** : 64
- **Nombre de couches** : 4 (encoder + decoder)

### Fonction de perte

**Oracle Mask Loss** : `L = || mask - oracle_mask ||â‚`

- `mask` : PrÃ©diction du modÃ¨le (entre 0 et 1)
- `oracle_mask` : VÃ©ritÃ© terrain = `vocals / (mix + eps)` (calculÃ© dans le domaine linÃ©aire)
- `|| ||â‚` : Norme L1 (somme des valeurs absolues des diffÃ©rences)

**Pourquoi cette mÃ©thode ?**
- Supervision directe : le modÃ¨le apprend directement Ã  prÃ©dire le bon masque
- Plus stable que de prÃ©dire le spectrogramme vocal directement
- Ã‰vite les problÃ¨mes de normalisation

---

## ğŸ“ Structure des Fichiers

```
projet_tds/
â”œâ”€â”€ data_generator.py      # Ã‰tape 1 : GÃ©nÃ©ration de donnÃ©es
â”œâ”€â”€ unet_model.py          # Ã‰tape 2 : Architecture du modÃ¨le
â”œâ”€â”€ train.py               # Ã‰tape 2 : Script d'entraÃ®nement
â”œâ”€â”€ inference.py           # Utilisation : SÃ©paration de voix
â”œâ”€â”€ requirements.txt       # DÃ©pendances Python
â”œâ”€â”€ README.md              # Ce fichier
â”‚
â”œâ”€â”€ checkpoints/           # ModÃ¨les entraÃ®nÃ©s
â”‚   â”œâ”€â”€ best_model.pth    # Meilleur modÃ¨le
â”‚   â””â”€â”€ logs/             # Logs TensorBoard
â”‚
â””â”€â”€ (fichiers audio gÃ©nÃ©rÃ©s)
```

---

## ğŸ” Explication des Concepts ClÃ©s

### Qu'est-ce qu'un spectrogramme ?

Imaginez une partition musicale : vous avez le temps en horizontal et les notes (frÃ©quences) en vertical. Un spectrogramme est similaire, mais au lieu de notes, vous avez l'intensitÃ© du son Ã  chaque frÃ©quence. Plus c'est brillant, plus le son est fort Ã  cette frÃ©quence.

### Qu'est-ce qu'un mask ?

Un mask est comme un "filtre" qui dit "Ã  chaque point du spectrogramme, garde X% du son". Par exemple :
- Mask = 0.8 â†’ garde 80% du son Ã  ce point
- Mask = 0.2 â†’ garde 20% du son Ã  ce point
- Mask = 0.0 â†’ supprime complÃ¨tement le son Ã  ce point

Le modÃ¨le apprend Ã  prÃ©dire ce mask pour extraire uniquement la voix.

### Pourquoi U-Net ?

U-Net est une architecture qui :
1. **RÃ©duit** d'abord la taille (encoder) pour comprendre les grandes structures
2. **Agrandit** ensuite (decoder) pour reconstruire les dÃ©tails
3. Utilise des **skip connections** pour prÃ©server les dÃ©tails fins

C'est comme regarder une photo de loin pour comprendre la composition, puis zoomer pour voir les dÃ©tails.

### Pourquoi Oracle Mask ?

Au lieu de dire au modÃ¨le "voici le spectrogramme vocal que tu dois produire", on lui dit "voici le masque que tu dois prÃ©dire". C'est plus direct et plus stable.

**Oracle mask** = "masque parfait" calculÃ© Ã  partir des donnÃ©es rÃ©elles : `vocals / mix`

---

## âš ï¸ ProblÃ¨mes Courants

### Erreur : "ffmpeg or ffprobe could not be found"

**Solution** : Installer ffmpeg
```bash
sudo apt-get install -y ffmpeg
```

### Erreur : "ModuleNotFoundError: No module named 'torch'"

**Solution** : Installer PyTorch
```bash
pip install torch torchvision torchaudio
```

### Le loss ne diminue pas

**Causes possibles** :
- Taux d'apprentissage trop Ã©levÃ© â†’ rÃ©duire `--lr` Ã  1e-4
- DonnÃ©es mal normalisÃ©es â†’ vÃ©rifier `data_generator.py`
- ModÃ¨le trop petit â†’ augmenter `n_channels` dans `unet_model.py`

### MÃ©moire insuffisante

**Solutions** :
- RÃ©duire `--batch-size` (ex: 8 au lieu de 16)
- RÃ©duire `n_channels` dans le modÃ¨le (ex: 32 au lieu de 64)

---

## ğŸ“ˆ RÃ©sultats Attendus

AprÃ¨s l'entraÃ®nement, vous devriez voir :
- **Loss initiale** : ~0.3-0.4
- **Loss finale** : ~0.1-0.15 (si le modÃ¨le apprend bien)
- **Mask std** : > 0.15 (indique que le mask varie, pas constant)

Le modÃ¨le sauvegardÃ© dans `checkpoints/best_model.pth` peut Ãªtre utilisÃ© pour sÃ©parer la voix de nouvelles chansons.

---

## ğŸ“š RÃ©fÃ©rences

- **Dataset** : MUSDB18 (https://sigsep.github.io/datasets/musdb.html)
- **Architecture** : U-Net (adaptÃ© pour la sÃ©paration de sources)
- **Papier de rÃ©fÃ©rence** : Voir `TP_M2SON (1).pdf`

---

## ğŸ¯ Objectifs du Projet (TP)

1. âœ… **Ã‰tape 1** : ImplÃ©menter la gÃ©nÃ©ration de donnÃ©es (spectrogrammes avec overlap)
2. âœ… **Ã‰tape 2** : ImplÃ©menter et entraÃ®ner le modÃ¨le U-Net
3. âœ… **Objectif** : Faire converger le modÃ¨le (pas nÃ©cessairement obtenir les meilleures performances)

**Note** : Ce projet est une version simplifiÃ©e pour l'apprentissage. Les performances peuvent Ãªtre amÃ©liorÃ©es avec plus de donnÃ©es, un modÃ¨le plus grand, et un entraÃ®nement plus long.

---

## ğŸ’¡ Conseils

- Commencez avec peu de chansons (5-10) pour tester rapidement
- Surveillez le loss : il devrait diminuer, pas augmenter
- Si le loss stagne, essayez de rÃ©duire le taux d'apprentissage
- Utilisez TensorBoard pour visualiser les courbes d'entraÃ®nement

---

**Bon entraÃ®nement ! ğŸµ**
